{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603ff7d2-9ff8-4a95-991c-d5c40af9f142",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Institute of Data - Capstone Project - Customer Churn Prediction (Banking Industry)\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "    <li><a href=\"#introduction\">Introduction</a></li>\n",
    "    <li><a href=\"#data_wrangling\">Data Wrangling</a></li>\n",
    "    <ul>\n",
    "        <li><a href=\"#data_dictionary\">Data Dictionary</a></li>\n",
    "    </ul>\n",
    "    <li><a href=\"#exploratory\">Exploratory Data Analysis</a></li>\n",
    "    <ul>\n",
    "        <li><a href=\"#categorical_eda\">Categorical EDA</a></li>\n",
    "        <li><a href=\"#numerical_eda\">Numerical EDA</a></li>\n",
    "        <li><a href=\"#customer_churn_eda\">Customer Churn EDA</a></li>\n",
    "    </ul>\n",
    "    <li><a href=\"#modeling\">Modeling</a></li>\n",
    "    <ul>\n",
    "        <li><a href=\"#pre-processing\">Pre-Processing</a></li>\n",
    "        <li><a href=\"#model_selection\">Model Selection</a></li>\n",
    "        <li><a href=\"#sampling_methods\">Sampling Methods</a></li>\n",
    "        <li><a href=\"#cross_validation\">Cross Validation</a></li>\n",
    "        <li><a href='#hyperparameter_tuning_and_model_building'>Hyperparameter Tuning and Model Building</a></li>\n",
    "        <li><a href=\"#feature_importance\">Feature Importance</a></li>\n",
    "    </ul>\n",
    "    <li><a href=\"#conclusion\">Conclusion</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03260b4e-4ca9-430b-b9c4-ba9845d256b9",
   "metadata": {},
   "source": [
    "## Introduction <a id='introduction'></a>\n",
    "\n",
    "Customer churn is the propensity of customers to cease doing business with a company in a given time period. It is an important metric in a business sense, as it's estimated that the cost of acquiring new customers being 5-6x greater than keeping existing ones. On this point alone, it makes sense for a business to implement strategies of keeping customers and reducing their likelihood of churning.\n",
    "\n",
    "The rise of digital and ‘neo-banks’ in the Australian banking space, has resulted in a renewed focus on customer churn within banks, as customers are presented with more options as to who they bank with. Coupled with the ease of online research and applications, switching banks has never been easier for customers. It makes sense given the costs of customer churn, that banks divert resources and attention into analysing their customers behaviour and employing Machine Learning algorithms to successfully predict those that’ll churn.\n",
    "\n",
    "This notebook will attempt to address the customer churn problem facing banks, by use of the following [dataset](https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers) from Kaggle. This problem will be addressed by building a predictive model, with the aim of successfully predicting whether a customer will churn or not. A bank would then be able to use this information to appropriately implement strategies to prevent the customer from churning, where successful implementation of a predictive model and subsequent strategy would see a reduction in costs, increased revenue, and increased customer satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d668e-4a41-4c76-a35f-550460d06dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODULES\n",
    "\n",
    "# Core Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Catboost\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Imblearn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# SkLearn\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "# Custom Modules\n",
    "import custom_code\n",
    "import helpers\n",
    "from show_summary_report import show_summary, show_summary_report\n",
    "\n",
    "# Show plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Pandas Options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Plot Styling\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684ddfa-3d9a-4b78-95d6-4f2c2ae731c0",
   "metadata": {},
   "source": [
    "## Data Wrangling <a id='data_wrangling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388830d-bfa5-49a0-b3e8-3f8b90e86980",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "df = pd.read_csv('dataset/BankChurners.csv')\n",
    "\n",
    "# Explore DataFrame\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# Shape\n",
    "print(f'Shape: {df.shape}')\n",
    "\n",
    "# Columns\n",
    "print('\\n**Columns**')\n",
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127ca82-8815-4321-861d-df256ad73964",
   "metadata": {},
   "source": [
    "Dataset contains 10,127 rows and 23 columns.\n",
    "\n",
    "The last 2 columns of this dataset should be ignored and will be dropped. I'll also drop the CLIENTNUM column as it serves no purpose in the analysis I'm going to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that aren't needed\n",
    "\n",
    "df.drop(df.columns[[0, 21, 22]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns\n",
    "\n",
    "df.columns = ['customer_churn', 'age', 'gender','num_dependents', 'education', 'marital_status', 'income', 'card_category',\n",
    "             'period_of_relationship','products_held', 'months_inactive', 'num_times_contacted', 'credit_limit', 'card_balance',\n",
    "             'open_to_buy_credit_avg', 'change_transaction_amount_Q4_Q1', 'transaction_amount', 'transaction_count',\n",
    "             'change_transaction_count_Q4_Q1', 'card_utilisation_ratio_avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f6f9e1-2842-40ff-b1c9-a685c70e2bf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Dictionary <a id='data_dictionary'></a>\n",
    "\n",
    "- **customer_churn**: Flag to show whether the client is current or has closed accounts with the bank\n",
    "- **age**: Customer Age in years\n",
    "- **gender**: M = Male, F = Female\n",
    "- **num_dependents**: Number of dependents the customer has\n",
    "- **education**: Highest educational attainment of customer\n",
    "- **marital_status**: Marital Status (Married/Single/Divorced/Unknown)\n",
    "- **income**: Annual income category the customer falls in\n",
    "- **card_category**: Type of card held (Blue, Silver, Gold, Platinum)\n",
    "- **period_of_relationship**: Period of relationship with bank\n",
    "- **products_held**: Number of products held by customer\n",
    "- **months_inactive**: Number of inactive months in the past 12 months\n",
    "- **num_times_contacted**: Number of times contacted in the last 12 months\n",
    "- **credit_limit**: Credit limit on customers credit card\n",
    "- **card_balance**: Total revolving balance on the credit card\n",
    "- **open_to_buy_credit_avg**: Open to buy credit line (average of last 12 months)\n",
    "- **change_transaction_amount_Q4_Q1**: Change in transaction amount (Q4 over Q1)\n",
    "- **transaction_amount**: Total transaction amount (last 12 months)\n",
    "- **transaction_count**: Number of transactions (last 12 months)\n",
    "- **change_transaction_count_Q4_Q1**: Change in transaction count (Q4 over Q1)\n",
    "- **card_utilisation_ratio_avg**: Average card utilisation ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57698651-fe87-414f-bb65-b3fe18995833",
   "metadata": {},
   "source": [
    "There is a good amount of information related to the customers in this dataset, that may have an impact as to whether the customer will churn from the bank. The target variable for this analysis is *customer_churn*, where I'll be coding 1 to represent a customer that has churned, and 0 to represent the converse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c804e2f-88b0-4030-91da-d9a00049585a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Info\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106b669-b02c-4cfe-b05d-b4353b0a44ed",
   "metadata": {},
   "source": [
    "There aren't any null values in this dataset, though I'll need to check if there are any values that appear incorrect. For example, a value of 0 in Age would be an incorrect value.\n",
    "\n",
    "In terms of data types, all variables are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2a7ee-9d62-4f98-9824-f5da4faab87e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Describe\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3808a-d16a-431f-b99e-050fac569ad4",
   "metadata": {},
   "source": [
    "Reviewing the above table, it appears as though there aren't any unexpected values in each numeric column. The min and max values of each column seem reasonable.\n",
    "\n",
    "Unfortunately, there is limited information on exactly what some of the columns mean. For example, the 'Avg_Open_To_Buy' column is quite ambiguous in its definition provided upon sourcing the dataset.\n",
    "\n",
    "I can dig into the data a little further, by exploring some of the following questions:\n",
    "\n",
    "- What is the relationship between Transaction Amount and Transaction Count, and how does this differ between current and churned customers?\n",
    "- Do customers with more products, spend more?\n",
    "\n",
    "This information may be useful in better understanding our customers spending habits, to assist in developing a marketing strategy to reduce customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140127c-189b-48bd-928f-f1d2088dd6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarise\n",
    "\n",
    "helpers.summarise(df).sort_values(by='n_unique', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b39a55",
   "metadata": {},
   "source": [
    "The above confirms again no null values, as well as shows that all the categorical variables will be useful as they have a low number of categories - Education Level is the highest with 7.\n",
    "\n",
    "Note that the *helpers.summarise* is a function from custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568fd5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Categorical Columns\n",
    "\n",
    "round(df.describe(exclude = ['float', 'int64']),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16ad5d",
   "metadata": {},
   "source": [
    "The above table tells me that a typical customer is current, female, graduate, married, earns less than $40k and has a Blue credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5d3f6-976e-4661-bbab-87b58be5d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Attrition_Flag to 0 = Current Customer & 1 = Churned Customer\n",
    "\n",
    "df.customer_churn = df.customer_churn.replace('Existing Customer', 0)\n",
    "df.customer_churn = df.customer_churn.replace('Attrited Customer', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a2ff9",
   "metadata": {},
   "source": [
    "The above was done to convert the target variable column to 0 and 1 values. This is required for machine learning models. 0 represents current customer and 1 represents churned customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512d7c7",
   "metadata": {},
   "source": [
    "Below I'll extract out the numeric and categorical column names, as this will be useful for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns with unique values <= 7\n",
    "\n",
    "discrete_cols = [col for col in df.columns if df[col].nunique() <= 7]\n",
    "discrete_cols.remove('customer_churn')\n",
    "\n",
    "# List of all other columns\n",
    "contin_cols = [col for col in df.columns if col not in discrete_cols]\n",
    "contin_cols.remove('customer_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab2cbb",
   "metadata": {},
   "source": [
    "To understand if I need to do any data wrangling on the categorical columns, I first need to see what values make up these variables. I'll loop through the categorical columns and display value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fbdde9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through categorical columns and display value counts\n",
    "\n",
    "for col in discrete_cols:\n",
    "    display(df[col].value_counts())\n",
    "    print('-'*35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f054aed",
   "metadata": {},
   "source": [
    "There are many 'Unknown' values in 3/5 of the categorical columns. I'll leave these as is for now and first see what sort of performance I get with the models. This may be an area to explore for future improvement, I could look at Imputing the 'Unknown' values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a735e-701d-4050-8bc4-38c9372fb482",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis <a id='exploratory'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bd211-6c2f-4e6f-844f-40fe2780f435",
   "metadata": {},
   "source": [
    "The target variable in this analysis is the 'customer_churn' column, which tells me which customers are current and which ones have churned. I'll begin by understanding the split between these two values and will achieve this using a bar plot.\n",
    "\n",
    "#### Target Variable - Churned Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df408a6-9bd3-4918-b56b-d760c8cff7d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Figure & Axes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# Draw Countplot\n",
    "\n",
    "sns.countplot(data=df, x='customer_churn', ax=ax)\n",
    "\n",
    "# Formatting\n",
    "\n",
    "ax.set_title('Number of Churned Customers', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.set_xticklabels(['Current Customer', 'Churned Customer'])\n",
    "sns.despine(fig=fig, bottom=True)\n",
    "\n",
    "# Show Plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Table\n",
    "\n",
    "helpers.find_frequency(df.customer_churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550627b-9544-4321-aacf-b16e6a398daa",
   "metadata": {},
   "source": [
    "The data I'm working with has a much larger number of current customers, relative to number of customers that have churned (84% vs 16%). The imbalance may impact the performance of the baseline models. If that is the case, it can be addressed using Under or Over Sampling techniques.\n",
    "\n",
    "Let's now get an overview of all the feature variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ce91e",
   "metadata": {},
   "source": [
    "### Categorical EDA <a id='categorical_eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb0d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate figure\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Loop through the length of columns and keep track of index\n",
    "\n",
    "for n, col in enumerate(discrete_cols):\n",
    "    # Add a new subplot iteratively\n",
    "    ax = plt.subplot(7,2, n+1)\n",
    "    \n",
    "    # Plot histogram on new subplot\n",
    "    sns.countplot(data=df, x=col, ax=ax,\n",
    "                 order = df[col].value_counts().index)\n",
    "    \n",
    "    # chart formatting\n",
    "    ax.set_title(col.upper())\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171ccd9",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "- Close to even split of Male and Female Customers\n",
    "- Most customers have at least one dependent\n",
    "- Multiple education levels, with Graduate being the most common\n",
    "- Most customers are Married or Single\n",
    "- Most customers earn less than \\$40k\n",
    "- Most customers are on the 'Blue' Card\n",
    "- 3 Products is the most common holding for customers\n",
    "- 3 months is the most common number of months for a customer to be inactive. Small number in >4 months.\n",
    "- The bank tends to not contact customers more than 4 times in a 12-month period\n",
    "\n",
    "I will need to encode these variables prior to training any machine learning models. One-Hot Encoding can be used on Gender & Martial Status and Ordinal Encoding on Education, Income and Card Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a662e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plots for Slides\n",
    "relevant_cols = ['gender', 'education', 'marital_status', 'income']\n",
    "\n",
    "# Instantiate figure\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Loop through the length of columns and keep track of index\n",
    "\n",
    "for n, col in enumerate(relevant_cols):\n",
    "    # Add a new subplot iteratively\n",
    "    ax = plt.subplot(7,2, n+1)\n",
    "    \n",
    "    # Plot histogram on new subplot\n",
    "    sns.countplot(data=df, x=col, ax=ax,\n",
    "                 order = df[col].value_counts().index)\n",
    "    \n",
    "    # chart formatting\n",
    "    ax.set_title(col.upper())\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00090f4b",
   "metadata": {},
   "source": [
    "### Numerical EDA <a id='numerical_eda'></a>\n",
    "\n",
    "There are 15 numerical columns, which I'll initially explore using a combination of histograms and boxplots. This'll provide some insight as to their distribution shape, as well as any potential outliers. \n",
    "\n",
    "###### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ca8db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate figure\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Loop through the length of columns and keep track of index\n",
    "\n",
    "for n, col in enumerate(contin_cols):\n",
    "    # Add a new subplot iteratively\n",
    "    ax = plt.subplot(7,4, n+1)\n",
    "    \n",
    "    # Plot histogram on new subplot\n",
    "    sns.histplot(data=df, x=col, ax=ax)\n",
    "    \n",
    "    # chart formatting\n",
    "    ax.set_title(col.upper())\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd816c3",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "- Age is the most normally distributed out of all the numeric variables\n",
    "- Period of relationship shows that there is a substantial number of customers in this dataset that have been with the bank for 36 months. This may be due to a marketing push done by the bank around that time to acquire new customers.\n",
    "- Credit Limit & Open to buy credit have a very similar distribution. With what seems to be many outliers at their tails.\n",
    "- Lots of customers appear to have a low balance on their credit card, then the distribution is normal, with a spike at the tail. This spike may be the customers with a higher credit limit.\n",
    "- Change in transaction amount and count shows that most customers in this dataset transacted less in both amount and count, over the Q4 to Q1 period.\n",
    "- Total transaction amount displays a multi modal distribution, with 4 apparent peaks. This would be a good variable to explore further, to better understand customers spending habits. The transaction count distribution only has 2 peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa23aca",
   "metadata": {},
   "source": [
    "###### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901dc93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Instantiate figure\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Loop through the length of columns and keep track of index\n",
    "\n",
    "for n, col in enumerate(contin_cols):\n",
    "    # Add a new subplot iteratively\n",
    "    ax = plt.subplot(7,3, n+1)\n",
    "    \n",
    "    # Plot histogram on new subplot\n",
    "    sns.boxplot(data=df, x=col, ax=ax)\n",
    "    \n",
    "    # chart formatting\n",
    "    ax.set_title(col.upper())\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccdbfa",
   "metadata": {},
   "source": [
    "Boxplots are useful to visually observe the outliers present in a distribution. I can see from the above plots that some of the variables in this dataset contain many outliers. These outliers are mainly present in variables that describe a customer’s spending habits.\n",
    "\n",
    "Income and Spending habits tend to be highly skewed distributions, given the disparity and inequality in incomes amongst individuals. Given that, we'd expect to see outliers of this nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aaa835",
   "metadata": {},
   "source": [
    "### Customer Churn EDA <a id='customer_churn_eda'></a>\n",
    "\n",
    "The above graphs have given a sense of the distribution of each individual variable on its own. I'll now move to exploring how these variables relate to the target variable of whether a customer has churned or not.\n",
    "\n",
    "For the discrete variables, I'll look at the countplot and barplot side by side, showing the mean value of the target variable in the bar plot. This'll give a sense of whether the variable impacts the customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc23d0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in discrete_cols:\n",
    "    helpers.plot_discrete(df, col, 'customer_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557be34",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "- Customers with Doctorates appear the most likely to churn\n",
    "- Customers with 3 or more dependents are the least likely to churn\n",
    "- Customers inactive for 4 months are most likely to churn. Seems as though all customers have been inactive for at least 1 month.\n",
    "- Most customers that were contacted 6 times churned and the more times a customer was contacted, the higher chance of churning is present.\n",
    "\n",
    "This insight that contacting customers more may have an impact on their likelihood to churn is important, as it may assist in developing a strategy to prevent churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset customers contacted 5 or 6 times\n",
    "df_subset = df[(df.num_times_contacted == 6) | (df.num_times_contacted == 5)]\n",
    "\n",
    "# Value counts of target variable\n",
    "df_subset.customer_churn.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36057f88",
   "metadata": {},
   "source": [
    "Looking at only customers that have been contacted 5 or 6 times in the past month, the split of current and churned customers rises to roughly 50/50. It appears that some customers may get fed up with the excessive contact and be part of the reason for them leaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Figure & Axes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# Draw Countplot\n",
    "\n",
    "sns.countplot(data=df, x='num_times_contacted', hue='customer_churn', ax=ax)\n",
    "\n",
    "# Formatting\n",
    "\n",
    "ax.set_title('Excessive Client Contact May Influence Customer Churn', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Number of Times Contacted')\n",
    "ax.set_ylabel('Customer Count')\n",
    "sns.despine(fig=fig, bottom=True)\n",
    "\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.legend(handles, ['Current Customer', 'Churned Customer'], loc='upper right')\n",
    "\n",
    "# Show Plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a32e58",
   "metadata": {},
   "source": [
    "The above barplot shows this insight even further, the relative difference between current and churned customers for each contact numbers gets smaller as the number of times contacted increases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eeacbb",
   "metadata": {},
   "source": [
    "For the continuous variables, I'll look at the kdeplot and boxplot side by side, with the target variable separated out by colour. This will also allow me to see any relationship between customer churn and the respective variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79e7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in contin_cols:\n",
    "    helpers.plot_continuous(df, col, 'customer_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba7cbb",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "- Customers with lower revolving balances are more likely to churn.\n",
    "- Customers with lower transaction balances are more likely to churn. Though for customers with a balance of 7500-10000, churn is more likely to happen here.\n",
    "- Customers with lower transaction counts are more likely to churn.\n",
    "- Customers with lower utilisation are more likely to churn.\n",
    "\n",
    "I'll now explore some of the earlier questions I raised, to see if I can pull out any further interesting insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Cleaner Plot for presentation\n",
    "\n",
    "# Create Figure & Axes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Draw Countplot\n",
    "\n",
    "sns.kdeplot(data=df, x='transaction_amount', hue ='customer_churn', ax=ax, shade=True, common_norm = False)\n",
    "\n",
    "# Formatting\n",
    "\n",
    "ax.set_title('Distribution of Customers Transaction Amount by Churn', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('transaction amount', fontsize=14)\n",
    "ax.set_ylabel('density', fontsize=14)\n",
    "sns.despine(fig=fig, bottom=True)\n",
    "\n",
    "ax.legend(title='', labels=['Churned Customer', 'Current Customer'])\n",
    "\n",
    "# Show Plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c11df1",
   "metadata": {},
   "source": [
    "##### What is the relationship between Transaction Amount and Transaction Count, and how does this differ between current and churned customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac313810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Figure & Axes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Draw Countplot\n",
    "\n",
    "sns.scatterplot(data=df, x='transaction_amount', y='transaction_count' ,hue ='customer_churn', ax=ax)\n",
    "\n",
    "# Formatting\n",
    "\n",
    "ax.set_xlabel('Transaction Amount', fontsize=14)\n",
    "ax.set_ylabel('Transaction Count', fontsize=14)\n",
    "ax.set_title('Transaction Amount & Count - Does it differ between current and churned customers?',\n",
    "            fontsize=16, fontweight='bold')\n",
    "sns.despine(fig=fig, bottom=True)\n",
    "\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.legend(handles, ['Current Customer', 'Churned Customer'], loc='lower right')\n",
    "\n",
    "# Show Plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda0bb0",
   "metadata": {},
   "source": [
    "**Key Observations**\n",
    "- There is a clear distinction between the transaction activity for current customers and churned customers.\n",
    "- Churned customers on average appear to spend less and have less transactions than the current customers.\n",
    "- No customer has left the bank if they spend more than $12,000 roughly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d9915",
   "metadata": {},
   "source": [
    "#####  Do customers with more products, spend more? How does this look between current and churned customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a504b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Figure & Axes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Draw Countplot\n",
    "\n",
    "sns.stripplot(data=df, x='products_held', y='transaction_amount' , jitter=0.4, hue ='customer_churn', ax=ax)\n",
    "\n",
    "# Formatting\n",
    "\n",
    "ax.set_xlabel('Number of Products Held', fontsize=14)\n",
    "ax.set_ylabel('Transaction Amount', fontsize=14)\n",
    "ax.set_title('Do customers with more products spend more?', fontsize=16, fontweight='bold')\n",
    "sns.despine(fig=fig, bottom=True)\n",
    "\n",
    "handles, labels  =  ax.get_legend_handles_labels()\n",
    "ax.legend(handles, ['Current Customer', 'Churned Customer'], loc='upper right')\n",
    "\n",
    "# Show Plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4c114",
   "metadata": {},
   "source": [
    "**Key Observations**\n",
    "- Customers with 1 or 2 products, that spend less than roughly $3,000 are likely to churn. Looking at the graph, very few current customers show this activity.\n",
    "- Customers with more products appear to spend less than those with less products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6eb06",
   "metadata": {},
   "source": [
    "Next, I'll plot the Correlation Matrix, to get a sense of which variables appear to influence Customer Churn the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86decc5e",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013d1be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set Theme\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Calculate correlations\n",
    "\n",
    "corr = df.corr()\n",
    "\n",
    "# Mask for top triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(18, 18))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "           annot=np.round(corr.values,2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1064c",
   "metadata": {},
   "source": [
    "The above shows me that transaction_count, change_transaction_count_Q4_Q1 and card_balance have the strongest negative correlation with the target variable. Which indicates that customer activity may be the best predictor of whether they will churn or not, and it looks to have less bearing on features such as age, period of relationship with bank and credit limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd94086",
   "metadata": {},
   "source": [
    "Some key themes that can be taken out from this EDA, is that customers that show less activity using their products, are more likely to churn. This may be due to the client’s intention of winding up and is intentionally moving money and activity to another bank. Additionally, customers with a higher number of dependents are more likely to stay. This may be because a customer doesn't have the time to search for other options.\n",
    "\n",
    "Regardless of the reasons, it would be beneficial for this bank to set up systems of identifying these trends and applying some intervention to re-engage the customer. Though it should be noted that too much contact may be counterproductive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36163a-8926-4a58-a781-9b4af07ff455",
   "metadata": {},
   "source": [
    "## Modeling <a id='modeling'></a>\n",
    "\n",
    "### Pre-Processing <a id='pre-processing'></a>\n",
    "\n",
    "##### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3801b9",
   "metadata": {},
   "source": [
    "I'll apply Ordinal Encoding on education, card category and income. Placing the 'Unknown' variable as the lowest value, to signify least importance, and subsequently have the lowest impact on model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22223b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Educational Level - Set Categories\n",
    "\n",
    "ordinal = OrdinalEncoder(categories=[['Unknown', 'Uneducated', 'High School', \n",
    "                                      'College', 'Graduate', 'Post-Graduate',\n",
    "                                     'Doctorate']])\n",
    "\n",
    "# Fit on Dataframe\n",
    "\n",
    "df['education'] = ordinal.fit_transform(df[['education']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Card Category - Set Categories\n",
    "\n",
    "ordinal = OrdinalEncoder(categories=[['Blue', 'Silver', 'Gold', 'Platinum']])\n",
    "\n",
    "# Fit on Dataframe\n",
    "\n",
    "df['card_category'] = ordinal.fit_transform(df[['card_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income Category - Set Categories\n",
    "\n",
    "ordinal = OrdinalEncoder(categories=[['Unknown', 'Less than $40K', '$40K - $60K',\n",
    "                                     '$60K - $80K', '$80K - $120K', '$120K +']])\n",
    "\n",
    "# Fit on Dataframe\n",
    "\n",
    "df['income'] = ordinal.fit_transform(df[['income']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ca463",
   "metadata": {},
   "source": [
    "As gender is binary, I'll pre-process that column without the use of a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88335c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = df['gender'].apply(lambda x: 1 if x == 'M' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ccf76a",
   "metadata": {},
   "source": [
    "I'll apply One Hot Encoding on Marital Status. To do this, we'll use pandas get_dummies()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dummies\n",
    "\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c4241",
   "metadata": {},
   "source": [
    "###### Features and Target Variables\n",
    "\n",
    "Creating the Feature and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b00437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & Y\n",
    "\n",
    "y = df.customer_churn\n",
    "X = df.drop('customer_churn', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6834a52",
   "metadata": {},
   "source": [
    "##### Train Test Split\n",
    "\n",
    "The data will be split up, one split, known as the train set will be to train the model. The other set is the test set, which will be used to test the model.\n",
    "\n",
    "I'll split 70/30, with the use of stratify, which ensures an even split of target variable outcomes between the two splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                   random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2207e3",
   "metadata": {},
   "source": [
    "###### Standardise\n",
    "\n",
    "I'll standardise our data when training each model, by use of Pipeline() and StandardScaler()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cd051",
   "metadata": {},
   "source": [
    "### Model Selection <a id='model_selection'></a>\n",
    "\n",
    "The business problem is to successfully predict if a customer will churn. I need to consider what our model should focus on in terms of a performance metric.\n",
    "\n",
    "When the model predicts that a customer will churn, they should then be targeted with the appropriate strategy to reduce their chance of churning.\n",
    "\n",
    "Let's consider the *cost* of when the model predicts a False Positive and False Negative.\n",
    "\n",
    "**False Positive**\n",
    "\n",
    "The model would classify this customer to churn, the business would then target the customer with the strategy to prevent churn. Though if this client wasn't going to churn, the targeted strategy will most likely have little impact. \n",
    "\n",
    "**False Negative**\n",
    "\n",
    "The model wouldn't classify this customer to churn, the business would do nothing, the customer may proceed to churn. The business has now lost a customer, without any intervention.\n",
    "\n",
    "The cost of applying an approved strategy to a customer would be much smaller than the cost of losing the customer. Based on this, we should be looking to build a model that minimises the False Negatives. The performance metric for this is **Recall**. \n",
    "\n",
    "Though looking at the Recall score in isolation wouldn't be advised, as we still don't want a model that predicts many False Positives. As a result, we'll make use of the **F1** as the primary scoring metric. We can then seek to optimise the model for higher Recall, depending on the businesses’ appetite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2f16e",
   "metadata": {},
   "source": [
    "###### Baseline Models\n",
    "\n",
    "The below cell contains all the models we'll train for our initial benchmark and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Learners - Stacking\n",
    "\n",
    "base_learners = [\n",
    "                 ('XGBoost', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "                 ('AdaBoost', AdaBoostClassifier()),\n",
    "                 ('Random Forest', RandomForestClassifier(random_state=0))\n",
    "                ]\n",
    "\n",
    "# List of baseline models\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=0)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=0)),\n",
    "    ('Support Vector Machine', SVC(probability=True)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('AdaBoost', AdaBoostClassifier()),\n",
    "    ('XGBoost', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "    ('CatBoost', CatBoostClassifier(random_seed=0, verbose=False)),\n",
    "    ('ExtraTree', ExtraTreesClassifier()),\n",
    "    ('Stacking', StackingClassifier(estimators=base_learners, \n",
    "                                    final_estimator=CatBoostClassifier(random_seed=0, verbose=False)))\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740af5ec",
   "metadata": {},
   "source": [
    "I've created a function below that will be used to test each model against each other, while using different sampling methods. As we have an imbalanced dataset, it will be useful to test different sampling methods to see if there is an increase in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Function for model comparison\n",
    "\n",
    "def model_comparision(models, sampler=''):\n",
    "    \n",
    "    # Dictionary to hold scores\n",
    "    models_scores = {}\n",
    "    \n",
    "    # Loop through each model\n",
    "    for model in models:\n",
    "        \n",
    "        # Steps for pipeline - Include sampler if passed into function\n",
    "        if sampler:\n",
    "            steps = [('scaler', StandardScaler()), ('sampler', sampler), model]\n",
    "        else:\n",
    "            steps = [('scaler', StandardScaler()), model]\n",
    "            \n",
    "        # Create Pipeline\n",
    "        pipeline = Pipeline(steps)\n",
    "\n",
    "        # Train model\n",
    "        model_trained = pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model_trained.predict(X_test)\n",
    "        y_prob = model_trained.predict_proba(X_test)\n",
    "\n",
    "        # Store model and test recall in dictionary\n",
    "        models_scores[model[0]] = [np.round(metrics.f1_score(y_test, y_pred), 4),\n",
    "                                   np.round(metrics.recall_score(y_test, y_pred), 4),\n",
    "                                   np.round(metrics.precision_score(y_test, y_pred), 4),\n",
    "                                   np.round(metrics.roc_auc_score(y_test, y_prob[:,1]), 4),\n",
    "                                   np.round(metrics.accuracy_score(y_test, y_pred), 4),                                \n",
    "                                   ]\n",
    "        \n",
    "    # Convert dictionary to dataframe\n",
    "\n",
    "    model_scores_df = pd.DataFrame.from_dict(models_scores, orient='index', \n",
    "                                             columns=['F1', 'Recall', 'Precision', 'AUC', 'Accuracy'])\n",
    "\n",
    "    # Sort Values\n",
    "\n",
    "    model_scores_df = model_scores_df.sort_values(by='F1', ascending=False)\n",
    "    \n",
    "    # Plot Values\n",
    "    fig, axes = plt.subplots(figsize=(5,5))\n",
    "\n",
    "    sns.barplot(data=model_scores_df, \n",
    "                y=model_scores_df.index, \n",
    "                x='F1', \n",
    "                palette='BuPu_r', \n",
    "                ax=axes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(model_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84189e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Baseline\n",
    "\n",
    "print('*'*10 + ' Baseline Model Comparision ' + '*'*10)\n",
    "model_comparision(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418ab8f",
   "metadata": {},
   "source": [
    "CatBoost and XGBoost out of the box have scored very well on our test data, followed closely by the Stacking Ensemble I put together. Random Forest and AdaBoost have decent scores, though they perform much more poorly on Recall compared to the first three.\n",
    "\n",
    "Next, I'll apply some sampling methods to see if an improvement in baseline scores can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc999b",
   "metadata": {},
   "source": [
    "### Sampling Methods <a id='sampling_methods'></a>\n",
    "\n",
    "For this analysis, I'll test out 3 different sampling methods. All these methods are from the Imblearn packages.\n",
    "\n",
    "- RandomOverSampler\n",
    "- RandomUnderSampler\n",
    "- SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sampler Objects\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "smote = SMOTE(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f7df3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RandomOverSampler\n",
    "\n",
    "print('*'*10 + ' RandomOverSampler Model Comparision ' + '*'*10)\n",
    "model_comparision(models, ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d659d804",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RandomUnderSampler\n",
    "\n",
    "print('*'*10 + ' RandomUnderSampler Model Comparision ' + '*'*10)\n",
    "model_comparision(models, rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838ec6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "\n",
    "print('*'*10 + ' SMOTE Model Comparision ' + '*'*10)\n",
    "model_comparision(models, smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b890f",
   "metadata": {},
   "source": [
    "RandomOverSampling provided the largest impact on our baseline models. Looking at CatBoost, it's improved the test F1 score to 0.92, and recall to 0.92.\n",
    "\n",
    "Prior to picking which model to use, I'll use cross validation on the top 4 performers after applying Random Over Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20a359",
   "metadata": {},
   "source": [
    "### Cross Validation <a id='cross_validation'></a>\n",
    "\n",
    "Applying cross validation to the top 4 models above (CatBoost, XGBoost, Random Forest, AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary to store models\n",
    "\n",
    "models = {\n",
    "        'CatBoost': CatBoostClassifier(random_seed=0, verbose=False),\n",
    "        'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        'Random Forest': RandomForestClassifier(random_state=0),\n",
    "        'AdaBoost': AdaBoostClassifier()\n",
    "\n",
    "}\n",
    "\n",
    "# Loop through each model, printing average recall score for 5 fold cross validation\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    # Create Pipeline\n",
    "    \n",
    "    pipeline = Pipeline([('transformer', StandardScaler()), ('sampler', ros), ('estimator', models[model])])\n",
    "    \n",
    "    # Cross Validation - 5 Folds\n",
    "    \n",
    "    print(f'{model} - 5 Fold CV - Average F1')\n",
    "    print(np.mean(cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1')))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78b36a",
   "metadata": {},
   "source": [
    "Cross Validation confirms that the initial baseline scores weren't due to random chance on how the data was initially split. This gives me the confidence to make the decision to use CatBoost as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94139d2d",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning & Model Building <a id='hyperparameter_tuning_and_model_building'></a>\n",
    "\n",
    "Based on the above work, I'll be using CatBoost as the model to predict whether a customer will churn. This next section will be building it out entirely, including further fine tuning to improve the F1 and Recall score.\n",
    "\n",
    "I was able to find the optimal parameters using Optuna, which I ran in a separate notebook using Colab, as I kept running into an error in this notebook that I couldn’t solve. The below code is just highlighting how GridSearchCV can be used to find optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1d11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "cb = CatBoostClassifier(random_seed=0, verbose=False)\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('ros', ros), (\"cat\", cb)])\n",
    "\n",
    "# Parameters to search over\n",
    "\n",
    "param_grid = {\n",
    "    \"cat__depth\": [9, 10],\n",
    "    \"cat__learning_rate\": [0.04769487291726708, 0.06769487291726708],\n",
    "    \"cat__l2_leaf_reg\": [6.57433418306558, 8.57433418306558],\n",
    "    \"cat__boosting_type\": ['Plain', 'Ordered'],\n",
    "    \"cat__grow_policy\": ['SymmetricTree', 'Depthwise']\n",
    "}\n",
    "\n",
    "# Search\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, cv=3)\n",
    "\n",
    "# Fit\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Ouput\n",
    "\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b53a85",
   "metadata": {},
   "source": [
    "GridSearchCV has found the best parameters, detailed above. I'll use those parameters to build the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "\n",
    "cb = CatBoostClassifier(depth=10,\n",
    "                        learning_rate=0.06769487291726708,\n",
    "                        l2_leaf_reg=8.57433418306558,\n",
    "                        iterations=1000,\n",
    "                        boosting_type='Ordered',\n",
    "                        verbose=False,\n",
    "                        grow_policy='SymmetricTree'\n",
    "                           )\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), ('ros', ros), (\"cat\", cb)])\n",
    "\n",
    "# Fit Model\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffdfd7",
   "metadata": {},
   "source": [
    "Below is the output of the Train & Test scores, making use of a custom code script that calculates all metrics and plots the confusion matrix, precision-recall curve and ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64322173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Predictions\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "y_train_prob = pipe.predict_proba(X_train)\n",
    "\n",
    "# Display Train Results\n",
    "show_summary_report(y_train, y_train_pred, y_train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13983c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Predictions\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "y_test_prob = pipe.predict_proba(X_test)\n",
    "\n",
    "# Display Test Results\n",
    "show_summary_report(y_test, y_test_pred, y_test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc362b",
   "metadata": {},
   "source": [
    "The final model is scoring very well, recall is 0.9160 which is the metric that will be of most use to the business, as they don't want to miss reaching out to customers that are likely to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a9de7",
   "metadata": {},
   "source": [
    "### Feature Importance <a id='feature_importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf1cef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create Feature Importance DataFrame\n",
    "feat_imp = pd.DataFrame(pipe[2].feature_importances_, index=X.columns, columns=[\"Importance\"])\n",
    "\n",
    "# Sort Values\n",
    "feat_imp = feat_imp.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Instantiate Figure & Axes\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# Plot Barplot\n",
    "sns.barplot(data=feat_imp, y=feat_imp.index, x='Importance', palette='BuPu_r')\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('Feature Importance', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('%')\n",
    "ax.set_ylabel('Feature')\n",
    "sns.despine(fig=fig, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9eda36",
   "metadata": {},
   "source": [
    "The importance of features above is in line with the initial insights I discovered. The ability to predict whether a customer will churn or not, is related primarily to the amount of transaction activity they're making. The less activity a customer is making, the more likely they are to churn.\n",
    "\n",
    "This makes sense, as customers that aren't satisfied with their current offering will be seeking alternatives and may take some time in winding up their accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df878b-dfd7-4555-b4ef-2a2ea60351d2",
   "metadata": {},
   "source": [
    "## Conclusion <a id='conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83762c1b",
   "metadata": {},
   "source": [
    "I've been able to successfully train a model to predict whether a customer will churn or not with % Recall. This is a well performing model that can have significant value in a business sense. Successful implementation of this model will allow the relevant business stakeholders to use this information and apply strategies to these customers, with the aim of reducing their chance of churning.\n",
    "\n",
    "Further development of this work can be done with:\n",
    "- PCA\n",
    "- Imputing 'Unknown' values\n",
    "- Acquiring more Data\n",
    "- Further fine tuning to increase Recall at expense of Precision\n",
    "- Implementing automated system to work with this model in implementing the strategy to keep customers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
